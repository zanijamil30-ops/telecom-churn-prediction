# -*- coding: utf-8 -*-
"""telecom churn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kvamS9yctSfoR7IENzq61uATeFgnD5ds
"""

from google.colab import files
import pandas as pd

# Upload file
uploaded = files.upload()

# Load dataset
df = pd.read_csv(next(iter(uploaded)))

# Drop ID column
df.drop(columns=['CustomerID'], inplace=True)

# Check first few rows
df.head()

# Check missing values and data types
print(df.info())
print(df.isnull().sum())

# Handle missing TotalCharges (if any)
df['TotalCharges'] = df['TotalCharges'].replace(" ", np.nan).astype(float)
df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)

# Double-check missing values again
df.isnull().sum()

from sklearn.preprocessing import LabelEncoder

# Copy dataset
df_processed = df.copy()

# Label encode binary columns
label_cols = ['Gender', 'TechSupport', 'StreamingTV', 'OnlineSecurity', 'PaperlessBilling']
le = LabelEncoder()
for col in label_cols:
    df_processed[col] = le.fit_transform(df_processed[col])

# One-hot encode multi-category columns
df_processed = pd.get_dummies(df_processed,
                              columns=['ContractType', 'InternetService', 'PaymentMethod'],
                              drop_first=True)

# Show result
df_processed.head()

from sklearn.preprocessing import StandardScaler

# Scale numeric columns
scaler = StandardScaler()
num_cols = ['Age', 'Tenure', 'MonthlyCharges', 'TotalCharges']
df_processed[num_cols] = scaler.fit_transform(df_processed[num_cols])

# Check result
df_processed.head()

from sklearn.model_selection import train_test_split

# Split features and target
X = df_processed.drop(columns=['Churn'])
y = df_processed['Churn']

# Train-test split (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape, X_test.shape

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_lr))

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))

from xgboost import XGBClassifier

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb.fit(X_train, y_train)
y_pred_xgb = xgb.predict(X_test)

print("XGBoost Accuracy:", accuracy_score(y_test, y_pred_xgb))

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
from scipy.stats import zscore
import pandas as pd

# Models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
}

# Store results
results = []

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    results.append({"Model": name, "Accuracy": acc})

# Create dataframe and compute z-scores
results_df = pd.DataFrame(results)
results_df["Z-Score"] = zscore(results_df["Accuracy"])

print(results_df)

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression(max_iter=1000)

param_grid = [
    {'solver': ['liblinear'], 'penalty': ['l1', 'l2'], 'C': [0.01, 0.1, 1, 10, 100]},
    {'solver': ['lbfgs'], 'penalty': ['l2'], 'C': [0.01, 0.1, 1, 10, 100]},
    {'solver': ['saga'], 'penalty': ['l1', 'l2'], 'C': [0.01, 0.1, 1, 10, 100]}
]

grid = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid.fit(X_train, y_train)

print("Best Parameters:", grid.best_params_)
print("Best Accuracy:", grid.best_score_)

from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

rf = RandomForestClassifier(n_estimators=150, random_state=42)
gb = GradientBoostingClassifier(n_estimators=150, random_state=42)
svm = SVC(probability=True, kernel='rbf', C=1, gamma='scale', random_state=42)

ensemble2 = VotingClassifier(
    estimators=[('rf', rf), ('gb', gb), ('svm', svm)],
    voting='soft'
)

ensemble2.fit(X_train, y_train)
y_pred_ensemble2 = ensemble2.predict(X_test)

print("Improved Ensemble Accuracy:", accuracy_score(y_test, y_pred_ensemble2))

from sklearn.ensemble import StackingClassifier, RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Base models
base_models = [
    ('rf', RandomForestClassifier(n_estimators=150, random_state=42)),
    ('gb', GradientBoostingClassifier(n_estimators=150, random_state=42)),
    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))
]

# Meta-model (final estimator)
meta_model = LogisticRegression(max_iter=1000)

# Stacking ensemble
stack = StackingClassifier(estimators=base_models, final_estimator=meta_model, passthrough=True)
stack.fit(X_train, y_train)
y_pred_stack = stack.predict(X_test)

print("Stacking Ensemble Accuracy:", accuracy_score(y_test, y_pred_stack))

# ✅ Predict on test set
y_pred_final = stack.predict(X_test)

# ✅ Show first few predictions
print("Sample Predictions:", y_pred_final[:10])

# ✅ If you want probabilities (for churn likelihood)
y_proba_final = stack.predict_proba(X_test)[:, 1]
print("Sample Probabilities:", y_proba_final[:10])

# Example new customer data (same feature structure as X)
new_data = X_test.iloc[:5]  # or replace with your new data

# Predict churn (0 = No, 1 = Yes)
new_preds = stack.predict(new_data)
print("Predicted Churn:", new_preds)

import ipywidgets as widgets
from IPython.display import display
import pandas as pd

# --- Input widgets ---
gender = widgets.Dropdown(options=['Male', 'Female'], description='Gender:')
age = widgets.IntSlider(value=30, min=18, max=80, description='Age:')
tenure = widgets.IntSlider(value=12, min=0, max=72, description='Tenure:')
contract = widgets.Dropdown(options=['Month-to-month', 'One year', 'Two year'], description='Contract:')
internet = widgets.Dropdown(options=['DSL', 'Fiber optic', 'No'], description='Internet:')
monthly = widgets.FloatSlider(value=70, min=20, max=120, description='Monthly Charges:')
total = widgets.FloatSlider(value=1000, min=0, max=10000, description='Total Charges:')
payment = widgets.Dropdown(options=['Electronic check', 'Mailed check', 'Bank transfer', 'Credit card'], description='Payment:')
tech = widgets.Dropdown(options=['Yes', 'No'], description='Tech Support:')
streaming = widgets.Dropdown(options=['Yes', 'No'], description='Streaming TV:')
security = widgets.Dropdown(options=['Yes', 'No'], description='Online Security:')
paperless = widgets.Dropdown(options=['Yes', 'No'], description='Paperless Billing:')

predict_button = widgets.Button(description="Predict Churn", button_style='success')
output = widgets.Output()

# --- Prediction function ---
def predict_churn(b):
    # Build DataFrame from inputs
    input_data = pd.DataFrame({
        'Gender': [gender.value],
        'Age': [age.value],
        'Tenure': [tenure.value],
        'ContractType': [contract.value],
        'InternetService': [internet.value],
        'MonthlyCharges': [monthly.value],
        'TotalCharges': [total.value],
        'PaymentMethod': [payment.value],
        'TechSupport': [tech.value],
        'StreamingTV': [streaming.value],
        'OnlineSecurity': [security.value],
        'PaperlessBilling': [paperless.value]
    })

    # --- Apply same preprocessing as training ---
    # Label encode binary columns
    binary_cols = ['Gender', 'TechSupport', 'StreamingTV', 'OnlineSecurity', 'PaperlessBilling']
    for col in binary_cols:
        input_data[col] = input_data[col].map({'Yes': 1, 'No': 0, 'Male': 1, 'Female': 0})

    # One-hot encode categorical columns
    input_data = pd.get_dummies(input_data,
                                columns=['ContractType', 'InternetService', 'PaymentMethod'],
                                drop_first=True)

    # Align columns with training data
    input_data = input_data.reindex(columns=X_train.columns, fill_value=0)

    # --- Predict churn ---
    prediction = stack.predict(input_data)[0]
    prob = stack.predict_proba(input_data)[0][1]

    # --- Display result ---
    with output:
        output.clear_output()
        if prediction == 1:
            print(f"⚠️ Customer is likely to CHURN (Probability: {prob:.2f})")
        else:
            print(f"✅ Customer is likely to STAY (Probability: {prob:.2f})")

# Bind button click
predict_button.on_click(predict_churn)

# --- Display interface ---
display(gender, age, tenure, contract, internet, monthly, total,
        payment, tech, streaming, security, paperless, predict_button, output)

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier

# pick the raw df if present, otherwise try to reconstruct categories from df_processed
try:
    raw = df.copy()                # raw before encoding (exists if you kept it)
except NameError:
    # try to reconstruct simple category cols from df_processed
    raw = df_processed.copy()
    # revert binary encodings if present
    if set(['Gender','TechSupport','StreamingTV','OnlineSecurity','PaperlessBilling']).issubset(raw.columns):
        pass
    # reconstruct contract/internet/payment from one-hot if possible
    def reconstruct(col_prefix):
        cols = [c for c in raw.columns if c.startswith(col_prefix + "_")]
        if not cols:
            return None
        # find for each row the column with 1, else fallback to first category name
        cats = [c.replace(col_prefix + "_","") for c in cols]
        arr = raw[cols].idxmax(axis=1).str.replace(col_prefix + "_","")
        return arr.replace({name: name for name in cats})
    if 'ContractType' not in raw.columns:
        ct = reconstruct("ContractType")
        if ct is not None: raw['ContractType'] = ct
    if 'InternetService' not in raw.columns:
        isv = reconstruct("InternetService")
        if isv is not None: raw['InternetService'] = isv
    if 'PaymentMethod' not in raw.columns:
        pm = reconstruct("PaymentMethod")
        if pm is not None: raw['PaymentMethod'] = pm

# (1) Target distribution
plt.figure(figsize=(5,3))
raw['Churn'].value_counts().plot(kind='bar')
plt.title("Churn distribution (0=No,1=Yes)"); plt.ylabel("Count"); plt.show()

# (2) Numeric features vs Churn (boxplots) — gives strong insight on differences
for col in ['Tenure','MonthlyCharges','TotalCharges','Age']:
    if col in raw.columns:
        plt.figure(figsize=(5,3))
        raw.boxplot(column=col, by='Churn', grid=False)
        plt.suptitle("")
        plt.title(f"{col} by Churn")
        plt.xlabel("Churn")
        plt.show()

# (3) Categorical churn rates for key categories (ContractType, InternetService, PaymentMethod)
for cat in ['ContractType','InternetService','PaymentMethod','Gender']:
    if cat in raw.columns:
        rates = raw.groupby(cat)['Churn'].mean().sort_values(ascending=False)
        plt.figure(figsize=(6,3))
        rates.plot(kind='bar')
        plt.title(f"Churn rate by {cat}"); plt.ylabel("Churn rate"); plt.show()

# (4) Correlation of numeric features with Churn (pearson)
num = raw.select_dtypes(include=[np.number])
if 'Churn' in num.columns and num.shape[1]>1:
    corr = num.corr()['Churn'].sort_values(ascending=False)
    print("Top correlations with Churn:\n", corr.head(10))

# (5) Feature importance from a simple RandomForest trained on processed data (works with df_processed/X_train)
if 'X_train' in globals() and 'y_train' in globals():
    rf = RandomForestClassifier(n_estimators=200, random_state=42)
    rf.fit(X_train, y_train)
    imp = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False).head(15)
    plt.figure(figsize=(8,4))
    imp.plot(kind='bar')
    plt.title("Top 15 Feature Importances (RandomForest)"); plt.ylabel("Importance"); plt.show()
    print("\nTop features:\n", imp)
else:
    print("Skipping feature-importance: X_train / y_train not found in namespace.")
